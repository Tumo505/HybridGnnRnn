{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "560d2f30",
   "metadata": {},
   "source": [
    "# Data Exploration and Initial Setup\n",
    "\n",
    "This notebook explores the key datasets for the hybrid GNN-RNN framework and sets up the data preprocessing pipeline optimized for M1 MacBook Pro with 16GB RAM.\n",
    "\n",
    "## Objectives\n",
    "1. Explore temporal data (GSE175634) for RNN training\n",
    "2. Examine spatial transcriptomics data for GNN training\n",
    "3. Assess data quality and memory requirements\n",
    "4. Set up memory-optimized preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9038098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import anndata as adata\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Memory optimization for M1 Mac\n",
    "import psutil\n",
    "import gc\n",
    "from memory_profiler import profile\n",
    "\n",
    "# Set scanpy settings for memory efficiency\n",
    "sc.settings.verbosity = 3\n",
    "sc.settings.set_figure_params(dpi=80, facecolor='white')\n",
    "\n",
    "# Check system resources\n",
    "def check_system_resources():\n",
    "    memory = psutil.virtual_memory()\n",
    "    cpu_count = psutil.cpu_count()\n",
    "    print(f\"Available RAM: {memory.available / (1024**3):.2f} GB\")\n",
    "    print(f\"Total RAM: {memory.total / (1024**3):.2f} GB\")\n",
    "    print(f\"CPU cores: {cpu_count}\")\n",
    "    print(f\"Platform: {sys.platform}\")\n",
    "\n",
    "check_system_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d58d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset catalog\n",
    "with open('../data_catalog/datasets.yaml', 'r') as f:\n",
    "    datasets_config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Available datasets:\")\n",
    "for category, datasets in datasets_config['datasets'].items():\n",
    "    print(f\"\\n{category.upper()}:\")\n",
    "    for name, info in datasets.items():\n",
    "        print(f\"  - {info['name']}\")\n",
    "        print(f\"    Priority: {info['priority']}\")\n",
    "        print(f\"    Platform: {info['platform']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee93fc4",
   "metadata": {},
   "source": [
    "## 1. Temporal Data Exploration (GSE175634)\n",
    "\n",
    "This is our primary dataset for RNN training with 7 timepoints and 230K+ cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418e3ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore temporal data structure\n",
    "temporal_path = \"../data/selected_datasets/temporal_data/\"\n",
    "\n",
    "# Load experimental design\n",
    "exp_design = pd.read_csv(\n",
    "    os.path.join(temporal_path, \"GSE175634_experimental_design.txt.gz\"), \n",
    "    sep='\\t'\n",
    ")\n",
    "\n",
    "print(\"Experimental Design Overview:\")\n",
    "print(f\"Shape: {exp_design.shape}\")\n",
    "print(\"\\nColumns:\", exp_design.columns.tolist())\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "print(exp_design.head(10))\n",
    "\n",
    "# Analyze time points\n",
    "timepoint_counts = exp_design['Day'].value_counts().sort_index()\n",
    "print(f\"\\nTimepoint distribution:\")\n",
    "print(timepoint_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b605c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cell metadata (memory-optimized)\n",
    "cell_metadata = pd.read_csv(\n",
    "    os.path.join(temporal_path, \"GSE175634_cell_metadata.tsv.gz\"), \n",
    "    sep='\\t',\n",
    "    nrows=1000  # Load first 1000 rows for exploration\n",
    ")\n",
    "\n",
    "print(\"Cell Metadata Overview:\")\n",
    "print(f\"Shape (first 1000 rows): {cell_metadata.shape}\")\n",
    "print(\"\\nColumns:\", cell_metadata.columns.tolist())\n",
    "print(\"\\nCell types available:\")\n",
    "print(cell_metadata['type'].value_counts())\n",
    "\n",
    "# Check for key differentiation markers\n",
    "print(\"\\nKey columns for analysis:\")\n",
    "for col in ['diffday', 'type', 'dpt_pseudotime', 'S.Score', 'G2M.Score']:\n",
    "    if col in cell_metadata.columns:\n",
    "        print(f\"  - {col}: {cell_metadata[col].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f35ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-efficient loading of count matrix metadata\n",
    "gene_indices = pd.read_csv(\n",
    "    os.path.join(temporal_path, \"GSE175634_gene_indices_counts.tsv.gz\"),\n",
    "    sep='\\t',\n",
    "    nrows=100  # First 100 genes for exploration\n",
    ")\n",
    "\n",
    "print(\"Gene Information:\")\n",
    "print(f\"Shape (first 100 genes): {gene_indices.shape}\")\n",
    "print(f\"Columns: {gene_indices.columns.tolist()}\")\n",
    "print(\"\\nSample genes:\")\n",
    "print(gene_indices.head())\n",
    "\n",
    "# Check for key cardiac genes\n",
    "cardiac_genes = ['TNNT2', 'MYH6', 'MYH7', 'NKX2-5', 'GATA4', 'TBX5']\n",
    "if 'gene_name' in gene_indices.columns:\n",
    "    found_genes = gene_indices[gene_indices['gene_name'].isin(cardiac_genes)]\n",
    "    print(f\"\\nCardiac genes found: {found_genes['gene_name'].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac55320",
   "metadata": {},
   "source": [
    "## 2. Spatial Transcriptomics Data Exploration\n",
    "\n",
    "Exploring spatial datasets for GNN training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201a9969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore spatial data structure\n",
    "spatial_paths = {\n",
    "    'space_ranger': \"../data/selected_datasets/spatial_transcriptomics/Spatial Gene Expression dataset analyzed using Space Ranger 1.1.0/\",\n",
    "    'mi_rna': \"../data/selected_datasets/spatial_transcriptomics/All-snRNA-Spatial multi-omic map of human myocardial infarction/\",\n",
    "    'xenium': \"../data/selected_datasets/spatial_transcriptomics/In Situ Gene Expression dataset analyzed using Xenium Onboard Analysis 1.9.0/\"\n",
    "}\n",
    "\n",
    "for name, path in spatial_paths.items():\n",
    "    print(f\"\\n{name.upper()} Dataset:\")\n",
    "    if os.path.exists(path):\n",
    "        files = os.listdir(path)\n",
    "        print(f\"  Files found: {len(files)}\")\n",
    "        print(f\"  Key files: {[f for f in files if f.endswith(('.h5', '.csv', '.tsv', '.mtx'))][:5]}\")\n",
    "    else:\n",
    "        print(f\"  Path not found: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b3506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Space Ranger data (memory optimized)\n",
    "space_ranger_path = spatial_paths['space_ranger']\n",
    "\n",
    "if os.path.exists(space_ranger_path):\n",
    "    # Check for standard Space Ranger outputs\n",
    "    h5_files = [f for f in os.listdir(space_ranger_path) if f.endswith('.h5')]\n",
    "    csv_files = [f for f in os.listdir(space_ranger_path) if f.endswith('.csv')]\n",
    "    \n",
    "    print(\"Space Ranger Files:\")\n",
    "    print(f\"  H5 files: {h5_files}\")\n",
    "    print(f\"  CSV files: {csv_files}\")\n",
    "    \n",
    "    # Load metrics summary if available\n",
    "    metrics_files = [f for f in csv_files if 'metrics' in f.lower()]\n",
    "    if metrics_files:\n",
    "        metrics = pd.read_csv(os.path.join(space_ranger_path, metrics_files[0]))\n",
    "        print(f\"\\nMetrics Summary:\")\n",
    "        print(metrics.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75de2bc8",
   "metadata": {},
   "source": [
    "## 3. Memory Optimization Strategy\n",
    "\n",
    "Setting up memory-efficient data loading and processing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1ca76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory optimization utilities\n",
    "class MemoryOptimizer:\n",
    "    def __init__(self, max_cells=10000, max_genes=5000):\n",
    "        self.max_cells = max_cells\n",
    "        self.max_genes = max_genes\n",
    "        \n",
    "    def subsample_data(self, adata, n_cells=None, n_genes=None):\n",
    "        \"\"\"Subsample data for memory efficiency\"\"\"\n",
    "        if n_cells is None:\n",
    "            n_cells = min(self.max_cells, adata.n_obs)\n",
    "        if n_genes is None:\n",
    "            n_genes = min(self.max_genes, adata.n_vars)\n",
    "            \n",
    "        # Subsample cells\n",
    "        if adata.n_obs > n_cells:\n",
    "            sc.pp.subsample(adata, n_obs=n_cells)\n",
    "            \n",
    "        # Select highly variable genes\n",
    "        if adata.n_vars > n_genes:\n",
    "            sc.pp.highly_variable_genes(adata, n_top_genes=n_genes)\n",
    "            adata = adata[:, adata.var.highly_variable]\n",
    "            \n",
    "        return adata\n",
    "    \n",
    "    def memory_efficient_load(self, path, backed=True):\n",
    "        \"\"\"Load data with memory mapping when possible\"\"\"\n",
    "        try:\n",
    "            if backed and path.endswith('.h5ad'):\n",
    "                return sc.read_h5ad(path, backed='r')\n",
    "            else:\n",
    "                return sc.read_h5ad(path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {path}: {e}\")\n",
    "            return None\n",
    "\n",
    "# Initialize memory optimizer\n",
    "memory_opt = MemoryOptimizer(\n",
    "    max_cells=datasets_config['memory_optimization']['max_cells_per_batch'],\n",
    "    max_genes=datasets_config['memory_optimization']['max_genes_per_analysis']\n",
    ")\n",
    "\n",
    "print(\"Memory optimization strategy:\")\n",
    "print(f\"  Max cells per batch: {memory_opt.max_cells}\")\n",
    "print(f\"  Max genes per analysis: {memory_opt.max_genes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67df84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test memory usage with sample data\n",
    "def estimate_memory_usage(n_cells, n_genes, dtype='float32'):\n",
    "    \"\"\"Estimate memory usage for given data dimensions\"\"\"\n",
    "    bytes_per_element = np.dtype(dtype).itemsize\n",
    "    memory_gb = (n_cells * n_genes * bytes_per_element) / (1024**3)\n",
    "    return memory_gb\n",
    "\n",
    "# Calculate memory requirements for different scenarios\n",
    "scenarios = [\n",
    "    (10000, 5000, \"Small batch\"),\n",
    "    (50000, 10000, \"Medium batch\"), \n",
    "    (100000, 20000, \"Large batch\"),\n",
    "    (230787, 25000, \"Full GSE175634\")\n",
    "]\n",
    "\n",
    "print(\"Memory usage estimates:\")\n",
    "for n_cells, n_genes, desc in scenarios:\n",
    "    memory_gb = estimate_memory_usage(n_cells, n_genes)\n",
    "    print(f\"  {desc}: {n_cells} cells × {n_genes} genes = {memory_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53865b69",
   "metadata": {},
   "source": [
    "## 4. Data Quality Assessment\n",
    "\n",
    "Quick quality assessment of key datasets to inform preprocessing strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9682195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality control function\n",
    "def quick_qc_assessment(metadata_df, qc_config):\n",
    "    \"\"\"Perform quick QC assessment on metadata\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    if 'type' in metadata_df.columns:\n",
    "        cell_type_dist = metadata_df['type'].value_counts()\n",
    "        results['cell_types'] = cell_type_dist\n",
    "        print(\"Cell type distribution:\")\n",
    "        print(cell_type_dist)\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_data = metadata_df.isnull().sum()\n",
    "    results['missing_data'] = missing_data[missing_data > 0]\n",
    "    \n",
    "    if len(results['missing_data']) > 0:\n",
    "        print(f\"\\nMissing data columns:\")\n",
    "        print(results['missing_data'])\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load full cell metadata for QC (in chunks if needed)\n",
    "print(\"Loading full cell metadata for QC assessment...\")\n",
    "\n",
    "try:\n",
    "    # Load in chunks to manage memory\n",
    "    chunk_size = 50000\n",
    "    metadata_chunks = []\n",
    "    \n",
    "    for chunk in pd.read_csv(\n",
    "        os.path.join(temporal_path, \"GSE175634_cell_metadata.tsv.gz\"), \n",
    "        sep='\\t',\n",
    "        chunksize=chunk_size\n",
    "    ):\n",
    "        metadata_chunks.append(chunk[['cell', 'diffday', 'type', 'dpt_pseudotime']])\n",
    "        if len(metadata_chunks) >= 5:  # Limit to first 5 chunks for exploration\n",
    "            break\n",
    "    \n",
    "    sample_metadata = pd.concat(metadata_chunks, ignore_index=True)\n",
    "    print(f\"Loaded sample metadata: {sample_metadata.shape}\")\n",
    "    \n",
    "    # Perform QC assessment\n",
    "    qc_results = quick_qc_assessment(sample_metadata, datasets_config['qc_thresholds'])\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during QC assessment: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168cf47d",
   "metadata": {},
   "source": [
    "## 5. Next Steps\n",
    "\n",
    "Based on this exploration, we'll proceed with:\n",
    "\n",
    "1. **Memory-optimized preprocessing** using the strategies defined above\n",
    "2. **Batch processing** for large datasets (GSE175634)\n",
    "3. **Subsampling strategies** for model development phase\n",
    "4. **Quality control pipelines** for all datasets\n",
    "\n",
    "The next notebook will implement the preprocessing pipeline for each dataset type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7138bd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save exploration results for next notebook\n",
    "exploration_results = {\n",
    "    'temporal_data': {\n",
    "        'timepoints': timepoint_counts.to_dict() if 'timepoint_counts' in locals() else {},\n",
    "        'estimated_cells': 230787,\n",
    "        'key_columns': ['cell', 'diffday', 'type', 'dpt_pseudotime']\n",
    "    },\n",
    "    'memory_strategy': {\n",
    "        'max_cells_per_batch': memory_opt.max_cells,\n",
    "        'max_genes_per_analysis': memory_opt.max_genes,\n",
    "        'use_subsampling': True\n",
    "    },\n",
    "    'next_steps': [\n",
    "        'Implement preprocessing pipeline',\n",
    "        'Create memory-efficient data loaders',\n",
    "        'Develop GNN and RNN model architectures',\n",
    "        'Set up training pipeline'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "import json\n",
    "with open('../experiments/exploration_results.json', 'w') as f:\n",
    "    json.dump(exploration_results, f, indent=2)\n",
    "\n",
    "print(\"Exploration complete! Results saved to experiments/exploration_results.json\")\n",
    "print(\"\\nNext: Run preprocessing notebook (1_preprocess/)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
