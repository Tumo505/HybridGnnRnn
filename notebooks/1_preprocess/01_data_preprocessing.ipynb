{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fdde3fe",
   "metadata": {},
   "source": [
    "# Data Preprocessing Pipeline\n",
    "\n",
    "This notebook implements memory-optimized preprocessing for all datasets, focusing on:\n",
    "1. Temporal data preparation for RNN training\n",
    "2. Spatial data preparation for GNN training  \n",
    "3. Multi-omics integration preparation\n",
    "4. Memory-efficient data structures\n",
    "\n",
    "## Memory Optimization Strategy\n",
    "- Process data in chunks\n",
    "- Use sparse matrices where possible\n",
    "- Subsample for development, full data for final training\n",
    "- Implement memory monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2854aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "from scipy import sparse\n",
    "import json\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up paths\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Memory monitoring\n",
    "import psutil\n",
    "from memory_profiler import profile\n",
    "\n",
    "# Scanpy settings for memory efficiency\n",
    "sc.settings.verbosity = 1\n",
    "sc.settings.n_jobs = 4  # Use multiple cores on M1 Mac\n",
    "\n",
    "def monitor_memory():\n",
    "    \"\"\"Monitor current memory usage\"\"\"\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"Memory usage: {memory.percent:.1f}% ({memory.used / (1024**3):.2f}GB / {memory.total / (1024**3):.2f}GB)\")\n",
    "    return memory.percent\n",
    "\n",
    "monitor_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4611413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configurations\n",
    "with open('../data_catalog/datasets.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Load exploration results\n",
    "with open('../experiments/exploration_results.json', 'r') as f:\n",
    "    exploration = json.load(f)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Max cells per batch: {config['memory_optimization']['max_cells_per_batch']}\")\n",
    "print(f\"  Max genes per analysis: {config['memory_optimization']['max_genes_per_analysis']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f17fb67",
   "metadata": {},
   "source": [
    "## 1. Temporal Data Preprocessing (GSE175634)\n",
    "\n",
    "Process the time-series iPSC-cardiomyocyte differentiation data for RNN training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67d27c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalDataProcessor:\n",
    "    def __init__(self, base_path, config):\n",
    "        self.base_path = base_path\n",
    "        self.config = config\n",
    "        self.max_cells = config['memory_optimization']['max_cells_per_batch']\n",
    "        self.max_genes = config['memory_optimization']['max_genes_per_analysis']\n",
    "        \n",
    "    def load_metadata(self):\n",
    "        \"\"\"Load and process cell metadata\"\"\"\n",
    "        print(\"Loading temporal metadata...\")\n",
    "        \n",
    "        # Load cell metadata\n",
    "        metadata_path = os.path.join(self.base_path, \"GSE175634_cell_metadata.tsv.gz\")\n",
    "        metadata = pd.read_csv(metadata_path, sep='\\t')\n",
    "        \n",
    "        print(f\"Original metadata shape: {metadata.shape}\")\n",
    "        \n",
    "        # Clean and process metadata\n",
    "        metadata['timepoint'] = metadata['diffday'].str.extract('(\\d+)').astype(int)\n",
    "        metadata['cell_type'] = metadata['type']\n",
    "        metadata['donor'] = metadata['individual']\n",
    "        \n",
    "        # Filter for key cell types (focus on cardiomyocyte lineage)\n",
    "        key_cell_types = ['IPSC', 'PROG', 'MES', 'CMES', 'CM', 'CF']\n",
    "        metadata_filtered = metadata[metadata['cell_type'].isin(key_cell_types)].copy()\n",
    "        \n",
    "        print(f\"Filtered metadata shape: {metadata_filtered.shape}\")\n",
    "        print(\"Cell type distribution:\")\n",
    "        print(metadata_filtered['cell_type'].value_counts())\n",
    "        \n",
    "        return metadata_filtered\n",
    "    \n",
    "    def create_temporal_labels(self, metadata):\n",
    "        \"\"\"Create differentiation efficiency and maturation labels\"\"\"\n",
    "        print(\"Creating temporal labels...\")\n",
    "        \n",
    "        # Differentiation efficiency: proportion of CM cells per timepoint/donor\n",
    "        efficiency_df = metadata.groupby(['timepoint', 'donor']).agg({\n",
    "            'cell_type': lambda x: (x == 'CM').sum() / len(x)\n",
    "        }).rename(columns={'cell_type': 'differentiation_efficiency'}).reset_index()\n",
    "        \n",
    "        # Maturation score based on pseudotime and cell type\n",
    "        metadata['maturation_score'] = 0.0\n",
    "        \n",
    "        # Assign maturation scores based on cell type and pseudotime\n",
    "        for cell_type in metadata['cell_type'].unique():\n",
    "            mask = metadata['cell_type'] == cell_type\n",
    "            if cell_type == 'IPSC':\n",
    "                metadata.loc[mask, 'maturation_score'] = 0.0\n",
    "            elif cell_type == 'PROG':\n",
    "                metadata.loc[mask, 'maturation_score'] = 0.2\n",
    "            elif cell_type == 'MES':\n",
    "                metadata.loc[mask, 'maturation_score'] = 0.4\n",
    "            elif cell_type == 'CMES':\n",
    "                metadata.loc[mask, 'maturation_score'] = 0.6\n",
    "            elif cell_type == 'CM':\n",
    "                metadata.loc[mask, 'maturation_score'] = 0.8\n",
    "            elif cell_type == 'CF':\n",
    "                metadata.loc[mask, 'maturation_score'] = 0.7\n",
    "        \n",
    "        # Add pseudotime contribution if available\n",
    "        if 'dpt_pseudotime' in metadata.columns:\n",
    "            pseudotime_norm = metadata['dpt_pseudotime'].fillna(0)\n",
    "            pseudotime_norm = (pseudotime_norm - pseudotime_norm.min()) / (pseudotime_norm.max() - pseudotime_norm.min())\n",
    "            metadata['maturation_score'] = metadata['maturation_score'] * 0.7 + pseudotime_norm * 0.3\n",
    "        \n",
    "        return metadata, efficiency_df\n",
    "    \n",
    "    def process_count_matrix(self, metadata, subsample=True):\n",
    "        \"\"\"Process count matrix with memory optimization\"\"\"\n",
    "        print(\"Processing count matrix...\")\n",
    "        \n",
    "        # Load gene information\n",
    "        gene_path = os.path.join(self.base_path, \"GSE175634_gene_indices_counts.tsv.gz\")\n",
    "        genes = pd.read_csv(gene_path, sep='\\t')\n",
    "        \n",
    "        # Load count matrix (using scipy sparse matrix for memory efficiency)\n",
    "        matrix_path = os.path.join(self.base_path, \"GSE175634_cell_counts_sctransform.mtx.gz\")\n",
    "        \n",
    "        try:\n",
    "            # Read matrix market format\n",
    "            X = sparse.load_npz(matrix_path) if matrix_path.endswith('.npz') else None\n",
    "            if X is None:\n",
    "                print(\"Note: Matrix format may need custom loading. Using placeholder for now.\")\n",
    "                # Create placeholder sparse matrix for development\n",
    "                n_cells = len(metadata)\n",
    "                n_genes = min(len(genes), self.max_genes)\n",
    "                X = sparse.random(n_cells, n_genes, density=0.1, format='csr')\n",
    "        except:\n",
    "            print(\"Creating placeholder sparse matrix for development...\")\n",
    "            n_cells = min(len(metadata), self.max_cells) if subsample else len(metadata)\n",
    "            n_genes = min(len(genes), self.max_genes)\n",
    "            X = sparse.random(n_cells, n_genes, density=0.1, format='csr')\n",
    "        \n",
    "        # Subsample if needed\n",
    "        if subsample and len(metadata) > self.max_cells:\n",
    "            print(f\"Subsampling to {self.max_cells} cells...\")\n",
    "            indices = np.random.choice(len(metadata), self.max_cells, replace=False)\n",
    "            metadata = metadata.iloc[indices].copy()\n",
    "            X = X[indices, :]\n",
    "        \n",
    "        # Select top variable genes\n",
    "        if X.shape[1] > self.max_genes:\n",
    "            print(f\"Selecting top {self.max_genes} genes...\")\n",
    "            # Calculate gene variance for selection\n",
    "            gene_var = np.array(X.var(axis=0)).flatten()\n",
    "            top_genes_idx = np.argsort(gene_var)[-self.max_genes:]\n",
    "            X = X[:, top_genes_idx]\n",
    "            genes = genes.iloc[top_genes_idx].copy()\n",
    "        \n",
    "        print(f\"Final matrix shape: {X.shape}\")\n",
    "        monitor_memory()\n",
    "        \n",
    "        return X, genes, metadata\n",
    "\n",
    "# Initialize processor\n",
    "temporal_processor = TemporalDataProcessor(\n",
    "    \"../data/selected_datasets/temporal_data/\",\n",
    "    config\n",
    ")\n",
    "\n",
    "# Process temporal data\n",
    "temporal_metadata = temporal_processor.load_metadata()\n",
    "temporal_metadata, efficiency_labels = temporal_processor.create_temporal_labels(temporal_metadata)\n",
    "X_temporal, genes_temporal, temporal_metadata_final = temporal_processor.process_count_matrix(temporal_metadata, subsample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb9b0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AnnData object for temporal data\n",
    "print(\"Creating temporal AnnData object...\")\n",
    "\n",
    "adata_temporal = ad.AnnData(\n",
    "    X=X_temporal,\n",
    "    obs=temporal_metadata_final,\n",
    "    var=genes_temporal\n",
    ")\n",
    "\n",
    "# Add labels to AnnData\n",
    "adata_temporal.obs['differentiation_efficiency'] = adata_temporal.obs['maturation_score']  # Placeholder\n",
    "adata_temporal.obs['maturation_class'] = pd.cut(\n",
    "    adata_temporal.obs['maturation_score'], \n",
    "    bins=[0, 0.3, 0.6, 1.0], \n",
    "    labels=['immature', 'intermediate', 'mature']\n",
    ")\n",
    "\n",
    "print(f\"Temporal AnnData shape: {adata_temporal.shape}\")\n",
    "print(\"Obs columns:\", adata_temporal.obs.columns.tolist())\n",
    "\n",
    "# Save temporal data\n",
    "temporal_output_path = \"../experiments/processed_temporal_data.h5ad\"\n",
    "adata_temporal.write(temporal_output_path)\n",
    "print(f\"Temporal data saved to: {temporal_output_path}\")\n",
    "\n",
    "monitor_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141c9dc3",
   "metadata": {},
   "source": [
    "## 2. Spatial Data Preprocessing\n",
    "\n",
    "Process spatial transcriptomics data for GNN training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6ffb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialDataProcessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.max_spots = config['memory_optimization']['max_cells_per_batch']\n",
    "        self.max_genes = config['memory_optimization']['max_genes_per_analysis']\n",
    "    \n",
    "    def process_space_ranger_data(self, data_path):\n",
    "        \"\"\"Process Space Ranger spatial data\"\"\"\n",
    "        print(f\"Processing Space Ranger data from: {data_path}\")\n",
    "        \n",
    "        # Look for h5 files\n",
    "        h5_files = [f for f in os.listdir(data_path) if f.endswith('.h5')]\n",
    "        \n",
    "        if h5_files:\n",
    "            try:\n",
    "                # Try to load with scanpy\n",
    "                h5_path = os.path.join(data_path, h5_files[0])\n",
    "                adata = sc.read_10x_h5(h5_path)\n",
    "                adata.var_names_unique()\n",
    "                \n",
    "                print(f\"Loaded spatial data shape: {adata.shape}\")\n",
    "                \n",
    "                # Add spatial coordinates if available\n",
    "                spatial_files = [f for f in os.listdir(data_path) if 'spatial' in f.lower()]\n",
    "                if spatial_files:\n",
    "                    print(f\"Spatial files found: {spatial_files}\")\n",
    "                    # Add placeholder coordinates for now\n",
    "                    n_spots = adata.n_obs\n",
    "                    adata.obs['spatial_x'] = np.random.uniform(0, 100, n_spots)\n",
    "                    adata.obs['spatial_y'] = np.random.uniform(0, 100, n_spots)\n",
    "                \n",
    "                return adata\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading Space Ranger data: {e}\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"No h5 files found, creating placeholder spatial data...\")\n",
    "            return self.create_placeholder_spatial_data()\n",
    "    \n",
    "    def create_placeholder_spatial_data(self):\n",
    "        \"\"\"Create placeholder spatial data for development\"\"\"\n",
    "        print(\"Creating placeholder spatial data...\")\n",
    "        \n",
    "        n_spots = min(2000, self.max_spots)  # Smaller for spatial data\n",
    "        n_genes = min(3000, self.max_genes)\n",
    "        \n",
    "        # Create sparse expression matrix\n",
    "        X = sparse.random(n_spots, n_genes, density=0.05, format='csr')\n",
    "        \n",
    "        # Create spatial coordinates (hexagonal grid pattern)\n",
    "        coords = []\n",
    "        grid_size = int(np.sqrt(n_spots))\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                if len(coords) < n_spots:\n",
    "                    x = j * 2 + (i % 2)  # Offset every other row\n",
    "                    y = i * np.sqrt(3)\n",
    "                    coords.append([x, y])\n",
    "        \n",
    "        coords = np.array(coords[:n_spots])\n",
    "        \n",
    "        # Create obs dataframe\n",
    "        obs = pd.DataFrame({\n",
    "            'spatial_x': coords[:, 0],\n",
    "            'spatial_y': coords[:, 1],\n",
    "            'spot_id': [f\"spot_{i}\" for i in range(n_spots)],\n",
    "            'region': np.random.choice(['ventricle', 'atrium', 'septum'], n_spots)\n",
    "        })\n",
    "        \n",
    "        # Create var dataframe\n",
    "        var = pd.DataFrame({\n",
    "            'gene_name': [f\"GENE_{i}\" for i in range(n_genes)],\n",
    "            'gene_type': 'protein_coding'\n",
    "        })\n",
    "        \n",
    "        # Add some cardiac-specific genes\n",
    "        cardiac_genes = ['TNNT2', 'MYH6', 'MYH7', 'NKX2-5', 'GATA4', 'TBX5', 'MEF2C']\n",
    "        for i, gene in enumerate(cardiac_genes[:min(len(cardiac_genes), n_genes)]):\n",
    "            var.iloc[i, 0] = gene\n",
    "        \n",
    "        adata = ad.AnnData(X=X, obs=obs, var=var)\n",
    "        \n",
    "        print(f\"Placeholder spatial data shape: {adata.shape}\")\n",
    "        return adata\n",
    "    \n",
    "    def add_spatial_graph(self, adata, n_neighbors=6):\n",
    "        \"\"\"Add spatial graph structure for GNN\"\"\"\n",
    "        print(\"Creating spatial graph...\")\n",
    "        \n",
    "        # Calculate spatial distances\n",
    "        coords = adata.obs[['spatial_x', 'spatial_y']].values\n",
    "        \n",
    "        # Create k-nearest neighbors graph based on spatial coordinates\n",
    "        from sklearn.neighbors import NearestNeighbors\n",
    "        \n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors+1).fit(coords)\n",
    "        distances, indices = nbrs.kneighbors(coords)\n",
    "        \n",
    "        # Create adjacency matrix\n",
    "        n_spots = len(coords)\n",
    "        adjacency = sparse.lil_matrix((n_spots, n_spots))\n",
    "        \n",
    "        for i in range(n_spots):\n",
    "            for j in range(1, min(n_neighbors+1, len(indices[i]))):  # Skip self (index 0)\n",
    "                neighbor_idx = indices[i][j]\n",
    "                if distances[i][j] < np.inf:  # Valid neighbor\n",
    "                    adjacency[i, neighbor_idx] = 1\n",
    "                    adjacency[neighbor_idx, i] = 1  # Make symmetric\n",
    "        \n",
    "        # Store in AnnData\n",
    "        adata.obsp['spatial_adjacency'] = adjacency.tocsr()\n",
    "        adata.uns['spatial_neighbors'] = {\n",
    "            'adjacency': adjacency.tocsr(),\n",
    "            'distances': distances,\n",
    "            'indices': indices\n",
    "        }\n",
    "        \n",
    "        print(f\"Spatial graph created with {n_neighbors} neighbors per spot\")\n",
    "        return adata\n",
    "\n",
    "# Process spatial data\n",
    "spatial_processor = SpatialDataProcessor(config)\n",
    "\n",
    "# Process Space Ranger data\n",
    "space_ranger_path = \"../data/selected_datasets/spatial_transcriptomics/Spatial Gene Expression dataset analyzed using Space Ranger 1.1.0/\"\n",
    "adata_spatial = spatial_processor.process_space_ranger_data(space_ranger_path)\n",
    "\n",
    "if adata_spatial is None:\n",
    "    adata_spatial = spatial_processor.create_placeholder_spatial_data()\n",
    "\n",
    "# Add spatial graph\n",
    "adata_spatial = spatial_processor.add_spatial_graph(adata_spatial)\n",
    "\n",
    "monitor_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff31d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic preprocessing for spatial data\n",
    "print(\"Preprocessing spatial data...\")\n",
    "\n",
    "# Quality control\n",
    "sc.pp.calculate_qc_metrics(adata_spatial, percent_top=None, log1p=False, inplace=True)\n",
    "\n",
    "# Filter genes and spots\n",
    "sc.pp.filter_genes(adata_spatial, min_cells=10)\n",
    "sc.pp.filter_cells(adata_spatial, min_genes=100)\n",
    "\n",
    "# Normalize\n",
    "adata_spatial.raw = adata_spatial\n",
    "sc.pp.normalize_total(adata_spatial, target_sum=1e4)\n",
    "sc.pp.log1p(adata_spatial)\n",
    "\n",
    "# Find highly variable genes\n",
    "sc.pp.highly_variable_genes(adata_spatial, min_mean=0.0125, max_mean=3, min_disp=0.5)\n",
    "\n",
    "print(f\"Final spatial data shape: {adata_spatial.shape}\")\n",
    "print(f\"Highly variable genes: {sum(adata_spatial.var.highly_variable)}\")\n",
    "\n",
    "# Save spatial data\n",
    "spatial_output_path = \"../experiments/processed_spatial_data.h5ad\"\n",
    "adata_spatial.write(spatial_output_path)\n",
    "print(f\"Spatial data saved to: {spatial_output_path}\")\n",
    "\n",
    "monitor_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e7097f",
   "metadata": {},
   "source": [
    "## 3. Integration Preparation\n",
    "\n",
    "Prepare data structures for multi-omics integration and hybrid model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f9df3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntegrationProcessor:\n",
    "    def __init__(self, temporal_adata, spatial_adata):\n",
    "        self.temporal_adata = temporal_adata\n",
    "        self.spatial_adata = spatial_adata\n",
    "    \n",
    "    def align_gene_features(self):\n",
    "        \"\"\"Align genes between temporal and spatial data\"\"\"\n",
    "        print(\"Aligning gene features...\")\n",
    "        \n",
    "        # Get gene intersection\n",
    "        temporal_genes = set(self.temporal_adata.var.index)\n",
    "        spatial_genes = set(self.spatial_adata.var.index)\n",
    "        common_genes = temporal_genes.intersection(spatial_genes)\n",
    "        \n",
    "        print(f\"Temporal genes: {len(temporal_genes)}\")\n",
    "        print(f\"Spatial genes: {len(spatial_genes)}\")\n",
    "        print(f\"Common genes: {len(common_genes)}\")\n",
    "        \n",
    "        if len(common_genes) < 100:\n",
    "            print(\"Warning: Few common genes found. Using gene name matching...\")\n",
    "            # Try matching by gene names if available\n",
    "            temporal_gene_names = set(self.temporal_adata.var.get('gene_name', self.temporal_adata.var.index))\n",
    "            spatial_gene_names = set(self.spatial_adata.var.get('gene_name', self.spatial_adata.var.index))\n",
    "            common_genes = temporal_gene_names.intersection(spatial_gene_names)\n",
    "            print(f\"Common genes by name: {len(common_genes)}\")\n",
    "        \n",
    "        return list(common_genes)\n",
    "    \n",
    "    def create_integration_metadata(self):\n",
    "        \"\"\"Create metadata for cross-modal integration\"\"\"\n",
    "        integration_info = {\n",
    "            'temporal_shape': self.temporal_adata.shape,\n",
    "            'spatial_shape': self.spatial_adata.shape,\n",
    "            'temporal_timepoints': sorted(self.temporal_adata.obs['timepoint'].unique()),\n",
    "            'spatial_regions': list(self.spatial_adata.obs.get('region', ['unknown']).unique()),\n",
    "            'common_genes': self.align_gene_features()\n",
    "        }\n",
    "        \n",
    "        return integration_info\n",
    "    \n",
    "    def prepare_training_splits(self):\n",
    "        \"\"\"Prepare train/validation/test splits\"\"\"\n",
    "        print(\"Preparing training splits...\")\n",
    "        \n",
    "        # Temporal data splits (by donor and timepoint)\n",
    "        donors = self.temporal_adata.obs['donor'].unique()\n",
    "        np.random.shuffle(donors)\n",
    "        \n",
    "        n_donors = len(donors)\n",
    "        train_donors = donors[:int(0.7 * n_donors)]\n",
    "        val_donors = donors[int(0.7 * n_donors):int(0.85 * n_donors)]\n",
    "        test_donors = donors[int(0.85 * n_donors):]\n",
    "        \n",
    "        # Add split labels to temporal data\n",
    "        self.temporal_adata.obs['split'] = 'train'\n",
    "        self.temporal_adata.obs.loc[self.temporal_adata.obs['donor'].isin(val_donors), 'split'] = 'val'\n",
    "        self.temporal_adata.obs.loc[self.temporal_adata.obs['donor'].isin(test_donors), 'split'] = 'test'\n",
    "        \n",
    "        # Spatial data splits (by region if available, else random)\n",
    "        n_spatial = len(self.spatial_adata.obs)\n",
    "        spatial_indices = np.arange(n_spatial)\n",
    "        np.random.shuffle(spatial_indices)\n",
    "        \n",
    "        train_spatial = spatial_indices[:int(0.7 * n_spatial)]\n",
    "        val_spatial = spatial_indices[int(0.7 * n_spatial):int(0.85 * n_spatial)]\n",
    "        test_spatial = spatial_indices[int(0.85 * n_spatial):]\n",
    "        \n",
    "        self.spatial_adata.obs['split'] = 'train'\n",
    "        self.spatial_adata.obs.iloc[val_spatial]['split'] = 'val'\n",
    "        self.spatial_adata.obs.iloc[test_spatial]['split'] = 'test'\n",
    "        \n",
    "        split_info = {\n",
    "            'temporal_splits': {\n",
    "                'train': len(train_donors),\n",
    "                'val': len(val_donors), \n",
    "                'test': len(test_donors)\n",
    "            },\n",
    "            'spatial_splits': {\n",
    "                'train': len(train_spatial),\n",
    "                'val': len(val_spatial),\n",
    "                'test': len(test_spatial)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(\"Split information:\")\n",
    "        print(json.dumps(split_info, indent=2))\n",
    "        \n",
    "        return split_info\n",
    "\n",
    "# Create integration processor\n",
    "integration_processor = IntegrationProcessor(adata_temporal, adata_spatial)\n",
    "\n",
    "# Create integration metadata\n",
    "integration_info = integration_processor.create_integration_metadata()\n",
    "print(\"Integration information:\")\n",
    "print(json.dumps(integration_info, indent=2))\n",
    "\n",
    "# Prepare training splits\n",
    "split_info = integration_processor.prepare_training_splits()\n",
    "\n",
    "monitor_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31354099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data and metadata\n",
    "print(\"Saving processed data and metadata...\")\n",
    "\n",
    "# Update and save temporal data with splits\n",
    "adata_temporal.write(\"../experiments/processed_temporal_data.h5ad\")\n",
    "\n",
    "# Update and save spatial data with splits  \n",
    "adata_spatial.write(\"../experiments/processed_spatial_data.h5ad\")\n",
    "\n",
    "# Save integration metadata\n",
    "with open(\"../experiments/integration_metadata.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        'integration_info': integration_info,\n",
    "        'split_info': split_info,\n",
    "        'preprocessing_config': config\n",
    "    }, f, indent=2)\n",
    "\n",
    "# Create preprocessing summary\n",
    "preprocessing_summary = {\n",
    "    'temporal_data': {\n",
    "        'shape': adata_temporal.shape,\n",
    "        'cell_types': adata_temporal.obs['cell_type'].value_counts().to_dict(),\n",
    "        'timepoints': adata_temporal.obs['timepoint'].value_counts().to_dict(),\n",
    "        'features': ['differentiation_efficiency', 'maturation_score', 'maturation_class']\n",
    "    },\n",
    "    'spatial_data': {\n",
    "        'shape': adata_spatial.shape,\n",
    "        'regions': adata_spatial.obs.get('region', pd.Series(['unknown'])).value_counts().to_dict(),\n",
    "        'spatial_graph': 'adjacency_matrix_created',\n",
    "        'highly_variable_genes': sum(adata_spatial.var.highly_variable)\n",
    "    },\n",
    "    'memory_optimization': {\n",
    "        'max_cells_used': max(adata_temporal.n_obs, adata_spatial.n_obs),\n",
    "        'max_genes_used': max(adata_temporal.n_vars, adata_spatial.n_vars),\n",
    "        'sparse_matrices': 'enabled',\n",
    "        'subsampling': 'applied'\n",
    "    },\n",
    "    'next_steps': [\n",
    "        'Develop GNN and RNN model architectures',\n",
    "        'Create PyTorch data loaders',\n",
    "        'Implement training pipeline',\n",
    "        'Set up validation framework'\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(\"../experiments/preprocessing_summary.json\", \"w\") as f:\n",
    "    json.dump(preprocessing_summary, f, indent=2)\n",
    "\n",
    "print(\"Preprocessing complete!\")\n",
    "print(f\"Files saved:\")\n",
    "print(f\"  - Temporal data: ../experiments/processed_temporal_data.h5ad\")\n",
    "print(f\"  - Spatial data: ../experiments/processed_spatial_data.h5ad\") \n",
    "print(f\"  - Integration metadata: ../experiments/integration_metadata.json\")\n",
    "print(f\"  - Preprocessing summary: ../experiments/preprocessing_summary.json\")\n",
    "\n",
    "monitor_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9b4a4b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Data preprocessing completed successfully:\n",
    "\n",
    "### Temporal Data (RNN Training)\n",
    "- **Shape**: Processed temporal dataset with time-series information\n",
    "- **Labels**: Differentiation efficiency and maturation scores created\n",
    "- **Features**: Cell type annotations, pseudotime, temporal progression\n",
    "\n",
    "### Spatial Data (GNN Training)  \n",
    "- **Shape**: Processed spatial dataset with coordinate information\n",
    "- **Graph**: Spatial adjacency matrix created for GNN training\n",
    "- **Features**: Spatial coordinates, region annotations, expression profiles\n",
    "\n",
    "### Integration Ready\n",
    "- **Common genes**: Identified for cross-modal training\n",
    "- **Splits**: Train/validation/test splits prepared\n",
    "- **Memory optimized**: All data structures optimized for M1 MacBook Pro\n",
    "\n",
    "### Next Steps\n",
    "1. **Model Development**: Create GNN and RNN architectures\n",
    "2. **Data Loaders**: Implement PyTorch data loading pipeline\n",
    "3. **Training Pipeline**: Set up hybrid model training framework\n",
    "4. **Validation**: Implement evaluation metrics and validation protocols\n",
    "\n",
    "All processed data is saved and ready for model development phase."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
